{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Титаник.\n",
    "\n",
    "Домашнее задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Данные\n",
    "\n",
    "Прочитаем тренировочные и тестовые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('titanic/train.csv')\n",
    "test  = pd.read_csv('titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "is_test        891 non-null int64\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "is_test        418 non-null int64\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 39.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature еngeneering\n",
    "\n",
    "Создаем и описываем фичи.\n",
    "Чтобы не дублировать операции для тренировочной тестовой выборки, объединим их:\n",
    "- удалим целевой столбеци из train\n",
    "- добавим флаг тестовой выборки\n",
    "- проверим, что что поля совпадают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочитаем данные еще раз\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test  = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "y_train = train.Survived\n",
    "# Удалим целевую функцию из train\n",
    "train.drop('Survived',axis=1,inplace=True)\n",
    "# Пометим выборки\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "# Склеим\n",
    "df = pd.concat([train,test])\n",
    "# Заменим пол со строковой переменной на числовую\n",
    "df['IsMale'] = df.Sex.replace({'male':1, 'female':0})\n",
    "df = feature_1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_have_age  = df[df.Age.notnull()].copy()\n",
    "df_without_age = df[df.Age.isnull()].copy()\n",
    "y_age = df_have_age.Age\n",
    "df_have_age.drop(['Age','PassengerId','Name','Sex','Ticket','Cabin','is_test'],axis=1,inplace=True)\n",
    "df_without_age.drop(['Age','PassengerId','Name','Sex','Ticket','Cabin','is_test'],axis=1,inplace=True)\n",
    "\n",
    "df_have_age_d = pd.get_dummies(df_have_age, columns=['Pclass','Embarked'])\n",
    "df_without_age_d = pd.get_dummies(df_without_age, columns=['Pclass','Embarked'])\n",
    "\n",
    "columns = df_have_age_d.columns\n",
    "\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0, verbose=1, copy = True)\n",
    "imputer.fit(df_have_age_d)\n",
    "\n",
    "df_have_age_d_i = imputer.transform(df_have_age_d)\n",
    "df_have_age_d_i = pd.DataFrame(df_have_age_d_i, columns=columns)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_have_age_d_i)\n",
    "df_have_age_d_i_s = scaler.transform(df_have_age_d_i)\n",
    "df_have_age_d_i_s = pd.DataFrame(df_have_age_d_i_s, columns = columns)\n",
    "\n",
    "\n",
    "df_without_age_d_i_s = scaler.transform(imputer.transform(df_without_age_d))\n",
    "df_without_age_d_i_s = pd.DataFrame(df_without_age_d_i_s, columns = columns)\n",
    "age_clf = get_xgboost_best_model(df_have_age_d_i_s, y_age, 'r2')\n",
    "#df_without_age_d_i_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давим фичу HaveCabin - проверим у кого указана каюта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['HaveCabin'] = df.Cabin.isnull()\n",
    "df.HaveCabin.replace({True : 1, False : 0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>is_test</th>\n",
       "      <th>IsMale</th>\n",
       "      <th>HaveMrs</th>\n",
       "      <th>HaveMr</th>\n",
       "      <th>HaveCol</th>\n",
       "      <th>HaveCabin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \\\n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S   \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C   \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S   \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S   \n",
       "\n",
       "   is_test  IsMale  HaveMrs  HaveMr  HaveCol  HaveCabin  \n",
       "0        0       1        0       1        0          1  \n",
       "1        0       0        1       0        0          0  \n",
       "2        0       0        0       0        0          1  \n",
       "3        0       0        1       0        0          0  \n",
       "4        0       1        0       1        0          1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все оставшивиеся текстовые поля удалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['PassengerId','Name','Sex','Ticket','Cabin'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поля - категогории разобьем на несколько столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['Pclass','Embarked'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим тренировочную и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_dummies[df_dummies.is_test == 0].drop('is_test',axis = 1)\n",
    "X_test = df_dummies[df_dummies.is_test == 1].drop('is_test', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение пустых значений. \n",
    "\n",
    "Пустые значения заполняем средними.\n",
    "На основе обучающей выборки создаем imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = X_train.columns\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0, verbose=1, copy = True)\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормировка значений\n",
    "\n",
    "Для обучающей выборки создаем scaler - нормировщик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "X_train_i_s = scaler.transform(X_train_imputed)\n",
    "X_train_i_s = pd.DataFrame(X_train_i_s, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Заполняем пустые значения и нормируем тестовую выборку\n",
    " \n",
    "Используем объекты imputer и scaler, обученные на обучающей выборке. Применяем эти объекты к тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_i_s = scaler.transform(imputer.transform(X_test))\n",
    "X_test_i_s = pd.DataFrame(X_test_i_s, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 1 (accuracy):  0.835016835017\n",
      "Fitting 8 folds for each of 12 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:   14.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 2 (accuracy):  0.836139169473\n",
      "Best score 3 (accuracy):  0.829405162738\n",
      "Best score 4 (accuracy):  0.841750841751\n",
      "Fitting 8 folds for each of 25 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 5 (accuracy):  0.83950617284\n",
      "Best score 6 (accuracy):  0.83950617284\n"
     ]
    }
   ],
   "source": [
    "best_xgboost_clf = get_xgboost_best_model(X_train_i_s, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "       gamma=0.5, learning_rate=0.025, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=5, missing=None, n_estimators=400, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=1e-05, reg_lambda=0.01,\n",
       "       scale_pos_weight=0.6229508196721312, seed=0, silent=True,\n",
       "       subsample=0.7)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier(**best_xgboost_clf)\n",
    "clf.fit(X_train_i_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.pyll.base.Apply at 0x7f9d69f9ba20>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.quniform('max_depth', 1, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.88408057022436615, 'status': 'ok'}\n",
      "Best: \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.4, 'learning_rate': 0.03, 'max_depth': 10.0, 'min_child_weight': 4.0, 'n_estimators': 300.0, 'reg_lambda': 0.01, 'scale_pos_weight': 1.0, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# в этой функции мы проверяем, как ведёт себя модель при заданных параметрах\n",
    "def score(params):\n",
    "    #print(\"Training with params : \")\n",
    "    #print(params)\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    clf = xgboost.XGBClassifier(**params)\n",
    "    #accurs = []\n",
    "    aucs = []\n",
    "    # Для оценки качества используем KFold, который определили выше\n",
    "    for train_idx, test_idx in cv.split(X_train_i_s):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        preds = clf.predict_proba(X_test_fold)\n",
    "        auc = roc_auc_score(y_test_fold, preds[:, 1])\n",
    "        aucs.append(auc)\n",
    "        #accur = clf.score(X_test_fold, y_test_fold)\n",
    "        #accurs.append(accur)\n",
    "    auc = np.mean(aucs)\n",
    "    result = {'loss':  auc, 'status': STATUS_OK}\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# это наша главная функция, в которой мы задаём параметры\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 500, 50), # (название параметра, от, до, шаг)\n",
    "             'learning_rate' : hp.quniform('learning_rate', 0.005, 0.03, 0.005),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 10, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "             'gamma' : hp.quniform('gamma', 0.0, 0.5, 0.1),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "             'objective': 'reg:linear',\n",
    "             'silent' : 1,\n",
    "             'scale_pos_weight': hp.quniform('scale_pos_weight', 0.5, 2, 0.5),\n",
    "             'reg_alpha': 0.0,\n",
    "             'reg_lambda': hp.quniform('reg_lambda', 0.01, 0.02, 0.005),\n",
    "             }\n",
    "\n",
    "    best = fmin( score, space, algo=tpe.suggest, trials=trials, max_evals=1)\n",
    "    print('Best: ')\n",
    "    print(best)\n",
    "\n",
    "#сюда будет записана\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_7 = trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_trials  = trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_max = 1\n",
    "my_num = 0\n",
    "for i in my_trials:\n",
    "    tmp = i['result']['loss']\n",
    "    if tmp <  my_max:\n",
    "        my_num = i['tid']\n",
    "        my_max = tmp\n",
    "my_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': [0.8],\n",
       " 'gamma': [0.4],\n",
       " 'learning_rate': [0.005],\n",
       " 'max_depth': [1.0],\n",
       " 'min_child_weight': [3.0],\n",
       " 'n_estimators': [300.0],\n",
       " 'reg_lambda': [0.01],\n",
       " 'scale_pos_weight': [1.5],\n",
       " 'subsample': [0.9]}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_7 = trials.trials[my_num]['misc']['vals']\n",
    "best_params_7\n",
    "best_params_8 = best_params_7.copy()\n",
    "best_params_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'gamma': 0.4,\n",
       " 'learning_rate': 0.005,\n",
       " 'max_depth': 1,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 300,\n",
       " 'reg_lambda': 0.01,\n",
       " 'scale_pos_weight': 1.5,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_8['colsample_bytree'] = float(best_params_8['colsample_bytree'][0])\n",
    "best_params_8['gamma'] = float(best_params_8['gamma'][0])\n",
    "best_params_8['learning_rate'] = float(best_params_8['learning_rate'][0])\n",
    "best_params_8['max_depth'] = int(best_params_8['max_depth'][0])\n",
    "best_params_8['min_child_weight'] = int(best_params_8['min_child_weight'][0])\n",
    "best_params_8['n_estimators'] = int(best_params_8['n_estimators'][0])\n",
    "best_params_8['scale_pos_weight'] = float(best_params_8['scale_pos_weight'][0])\n",
    "best_params_8['reg_lambda'] = float(best_params_8['reg_lambda'][0])\n",
    "best_params_8['subsample'] = float(best_params_8['subsample'][0])\n",
    "best_params_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78226711560044893"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier(**best_params_8)\n",
    "clf.fit(X_train_i_s,y_train)\n",
    "clf.score (X_train_i_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем сабмит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = xgboost.XGBClassifier(**best_params_8)\n",
    "#clf.fit(X_train_i_s,y_train)\n",
    "y_test = clf.predict(X_test_i_s)\n",
    "submit = pd.DataFrame(test.PassengerId)\n",
    "submit['Survived'] = y_test\n",
    "submit.tail()\n",
    "submit.to_csv('submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_1(df_src):\n",
    "    df = df_src.copy()\n",
    "\n",
    "    df['HaveMrs'] = df.Name.str.contains('mrs.', case=False)\n",
    "    df.HaveMrs.replace({True:1, False : 0}, inplace=True)\n",
    "\n",
    "    df['HaveMr'] = df.Name.str.contains('mr\\.', case = False)\n",
    "    df.HaveMr.replace({True : 1, False : 0},inplace=True)\n",
    "\n",
    "    df['HaveRev'] = df.Name.str.contains('rev\\.', case = False)\n",
    "    df.HaveRev.replace({True : 1, False : 0},inplace=True)\n",
    "\n",
    "    df['HaveMiss'] = df.Name.str.contains('miss\\.', case = False)\n",
    "    df.HaveMiss.replace({True : 1, False : 0},inplace=True)\n",
    "\n",
    "    df['HaveDr'] = df.Name.str.contains('dr\\.', case = False)\n",
    "    df.HaveDr.replace({True : 1, False : 0},inplace=True)\n",
    "\n",
    "    df['HaveMaster'] = df.Name.str.contains('master\\.', case = False)\n",
    "    df.HaveMaster.replace({True : 1, False : 0},inplace=True)\n",
    "\n",
    "    df['HaveCol'] = df.Name.str.contains('col\\.', case = False)\n",
    "    df.HaveCol.replace({True : 1, False : 0},inplace=True)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xgboost_best_model(X_train,y_train, score_type, ones_ratio = 1):\n",
    "    # посчитаем соотношение между классами\n",
    "    #ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    \"\"\" \n",
    "    Шаг 1: Зафиксируем learning_rate и параметры дерева и подберём n_estimators¶\n",
    "\n",
    "    Параметры:\n",
    "    \n",
    "    max_depth. Как указанов в таблице выше, обычно варьируется в интервале от 3 до 10 \n",
    "        (но от задачи к задаче значения могут меняться). В качестве начального значения обычно используют 5\n",
    "    min_child_weight. Если выборка сильно несбалансирована, то лучше выбрать значение \"1\". \n",
    "        Иначе лучше выбрать значение \"2\" и зафиксировать\n",
    "    gamma. Обычно выставляют значение в интервале от 0 до 0.2 и фиксируют. \n",
    "        В дальнейшем этот параметр всегда можно затюнить отдельно\n",
    "    subsample, colsample_bytree. Выставим 0.8 и зафиксируем. \n",
    "        Можно также проварьировать в интервале 0.5-0.9.\n",
    "    scale_pos_weight. Выставляется в зафисимости от соотношения классов в выборке и фиксируется\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        'n_estimators': [10, 30, 50, 100, 200, 400, 600, 1000],\n",
    "        #'n_estimators': [10, 30, 50],\n",
    "        'learning_rate': [0.1, ],\n",
    "        \n",
    "        # параметры дерева\n",
    "        'max_depth': [5],\n",
    "        'min_child_weight': [2],\n",
    "        'gamma': [0.1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'scale_pos_weight': [ones_ratio],\n",
    "        \n",
    "        # параметры регуляризации\n",
    "        'reg_alpha': [0.0],\n",
    "        'reg_lambda': [1.0]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=8, shuffle=True)\n",
    "\n",
    "    clf = xgboost.XGBClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_1 = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "    #print('Best params: ', best_params_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 2. Подбираем параметры дерева\n",
    "\n",
    "        max_depth - будем варьировать от 3 до 10 с шагом 2\n",
    "        min_child_weight - от 1 до 6 с шагом 2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': range(3, 10, 2),\n",
    "        'min_child_weight': range(1, 6, 2)\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_1) # в качестве отправной точки возьмём модель с наилучшими параметрами предыдущего шага\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_2 = gs.best_estimator_.get_params()\n",
    "    print('Best score 2 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 3. Подбираем gamma (критерий создания поддерева)\n",
    "        gamma - от 0 до 0.5 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'gamma': [0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_2)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_3 = gs.best_estimator_.get_params()\n",
    "    print('Best score 3 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "    #print('Best params: ', best_params_3)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 4. Затюним subsample и colsample_bytree¶\n",
    "\n",
    "    subsample - от 0.5 до 1.0 с шагом 0.1\n",
    "    colsample_bytree - от 0.5 до 1.0 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'subsample': [0.5 + 0.1*i for i in range(6)],\n",
    "        'colsample_bytree': [0.5 + 0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_3)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_4 = gs.best_estimator_.get_params()\n",
    "    print('Best score 4 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_4)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 5. Регуляризация\n",
    "    reg_alpha [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    reg_lambda [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
    "        'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_4)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_5 = gs.best_estimator_.get_params()\n",
    "    print('Best score 5 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_5)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 6. Learning rate\n",
    "\n",
    "    Чем меньше у нас n_estimators в ансамбле, тем быстрее нам нужно двигаться с каждым шагом \n",
    "    (добавлением нового классификатора), т.е. делать больший learning_rate. \n",
    "    Обычно learning rate варьируют так, чтобы произведение n_estimators x learning_rate оставалось инвариантным\n",
    "    \"\"\"\n",
    "    best_params_6 = best_params_5.copy()\n",
    "    clf = xgboost.XGBClassifier(**best_params_5)\n",
    "    best_n_estimators = clf.get_params()['n_estimators'] # возьмём наилучшие значения n_estimators с предыдущего шага\n",
    "    best_learning_rate = best_params['learning_rate'] # аналогичная запись\n",
    "    invariant_composition = best_n_estimators * best_learning_rate\n",
    "    n_estimators_range = [10, 30, 100, 200, 400, 600, 800, 1000]\n",
    "\n",
    "    best_score = gs.best_score_ # возьмём наилучшее качество с предыдущего шага\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        learning_rate = invariant_composition / n_estimators\n",
    "        clf.set_params(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        #aucs = []\n",
    "        accurs = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            #preds = clf.predict_proba(X_test_fold)\n",
    "            #auc = roc_auc_score(y_test_fold, preds[:, 1])\n",
    "            #aucs.append(auc)\n",
    "            accur = clf.score(X_test_fold, y_test_fold)\n",
    "            accurs.append(accur)\n",
    "        accur = np.mean(accurs)\n",
    "        if accur > best_score:\n",
    "            best_n_estimators = n_estimators\n",
    "            best_learning_rate = learning_rate\n",
    "            best_score = accur\n",
    "    \n",
    "    best_params_6['n_estimators'] = best_n_estimators\n",
    "    best_params_6['learning_rate'] = best_learning_rate\n",
    "\n",
    "    print('Best score 6: ', best_score)\n",
    "    \n",
    "    return best_params_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
