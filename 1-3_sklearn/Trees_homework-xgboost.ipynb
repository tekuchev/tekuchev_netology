{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Титаник.\n",
    "\n",
    "Домашнее задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Данные\n",
    "\n",
    "Прочитаем тренировочные и тестовые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('titanic/train.csv')\n",
    "test  = pd.read_csv('titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Survived</td>     <th>  R-squared:         </th> <td>   0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   113.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 16 Nov 2017</td> <th>  Prob (F-statistic):</th> <td>9.83e-75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:27:42</td>     <th>  Log-Likelihood:    </th> <td> -328.83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   714</td>      <th>  AIC:               </th> <td>   667.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   709</td>      <th>  BIC:               </th> <td>   690.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>   <td>    1.3169</td> <td>    0.077</td> <td>   17.104</td> <td> 0.000</td> <td>    1.166</td> <td>    1.468</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sex[T.male]</th> <td>   -0.4787</td> <td>    0.031</td> <td>  -15.518</td> <td> 0.000</td> <td>   -0.539</td> <td>   -0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age</th>         <td>   -0.0054</td> <td>    0.001</td> <td>   -4.975</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Fare</th>        <td> 6.801e-05</td> <td>    0.000</td> <td>    0.205</td> <td> 0.838</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pclass</th>      <td>   -0.2004</td> <td>    0.023</td> <td>   -8.907</td> <td> 0.000</td> <td>   -0.245</td> <td>   -0.156</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>24.788</td> <th>  Durbin-Watson:     </th> <td>   1.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  26.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.473</td> <th>  Prob(JB):          </th> <td>1.61e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.046</td> <th>  Cond. No.          </th> <td>    365.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               Survived   R-squared:                       0.390\n",
       "Model:                            OLS   Adj. R-squared:                  0.387\n",
       "Method:                 Least Squares   F-statistic:                     113.4\n",
       "Date:                Thu, 16 Nov 2017   Prob (F-statistic):           9.83e-75\n",
       "Time:                        00:27:42   Log-Likelihood:                -328.83\n",
       "No. Observations:                 714   AIC:                             667.7\n",
       "Df Residuals:                     709   BIC:                             690.5\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "Intercept       1.3169      0.077     17.104      0.000       1.166       1.468\n",
       "Sex[T.male]    -0.4787      0.031    -15.518      0.000      -0.539      -0.418\n",
       "Age            -0.0054      0.001     -4.975      0.000      -0.008      -0.003\n",
       "Fare         6.801e-05      0.000      0.205      0.838      -0.001       0.001\n",
       "Pclass         -0.2004      0.023     -8.907      0.000      -0.245      -0.156\n",
       "==============================================================================\n",
       "Omnibus:                       24.788   Durbin-Watson:                   1.850\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.676\n",
       "Skew:                           0.473   Prob(JB):                     1.61e-06\n",
       "Kurtosis:                       3.046   Cond. No.                         365.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = smf.ols('Survived ~ Age + Fare + Pclass + Sex', train).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "is_test        418 non-null int64\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 39.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature еngeneering\n",
    "\n",
    "Создаем и описываем фичи.\n",
    "Чтобы не дублировать операции для тренировочной тестовой выборки, объединим их:\n",
    "- удалим целевой столбеци из train\n",
    "- добавим флаг тестовой выборки\n",
    "- проверим, что что поля совпадают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочитаем данные еще раз\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test  = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "y_train = train.Survived\n",
    "# Удалим целевую функцию из train\n",
    "train.drop('Survived',axis=1,inplace=True)\n",
    "# Пометим выборки\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "# Склеим\n",
    "df = pd.concat([train,test])\n",
    "# Заменим пол со строковой переменной на числовую\n",
    "df['IsMale'] = df.Sex.replace({'male':1, 'female':0})\n",
    "#Давим фичу HaveCabin - проверим у кого указана каюта.\n",
    "#df['HaveCabin'] = df.Cabin.isnull()\n",
    "#df.HaveCabin.replace({True : 1, False : 0},inplace=True)\n",
    "get_titles(df)\n",
    "process_age(df)\n",
    "process_ticket(df)\n",
    "process_family(df)\n",
    "process_cabin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['Sex','Pclass','Title'])\n",
    "grouped_median = grouped.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['FareLog'] = df.Fare.apply(np.log) \n",
    "#df.FareLog[df.Fare == 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.scatter(df.Age, df.FareLog, facecolors='None', edgecolors='grey') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = gs.best_estimator_\n",
    "#clf.fit(df_have_age_d_i_s, y_age)\n",
    "#y_pred = clf.predict(df_have_age_d_i_s)\n",
    "#for i,j in zip(y_age, y_pred):\n",
    "    #print (\"{}  {}\".format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
       "       'Ticket', 'Fare', 'Cabin', 'Embarked', 'is_test', 'IsMale', 'HaveCabin',\n",
       "       'Title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare',\n",
       "       'Cabin', 'Embarked', 'IsMale', 'HaveCabin', 'Title', 'Pclass_1',\n",
       "       'Pclass_2', 'Pclass_3', 'Survived'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_est = df.copy()\n",
    "df_est = df_est[df_est.is_test == 0].drop('is_test',axis = 1)\n",
    "df_est = pd.get_dummies(df_est, columns=['Pclass',])\n",
    "df_est['Survived'] = y_train\n",
    "#df_est.head()\n",
    "df_est.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Survived</td>     <th>  R-squared:         </th> <td>   0.441</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.434</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   62.99</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 16 Nov 2017</td> <th>  Prob (F-statistic):</th> <td>4.44e-103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:22:48</td>     <th>  Log-Likelihood:    </th> <td> -363.02</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   891</td>      <th>  AIC:               </th> <td>   750.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   879</td>      <th>  BIC:               </th> <td>   807.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>    1.4166</td> <td>    0.259</td> <td>    5.470</td> <td> 0.000</td> <td>    0.908</td> <td>    1.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Title[T.Miss]</th>    <td>   -0.5878</td> <td>    0.261</td> <td>   -2.254</td> <td> 0.024</td> <td>   -1.100</td> <td>   -0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Title[T.Mr]</th>      <td>   -0.4899</td> <td>    0.071</td> <td>   -6.894</td> <td> 0.000</td> <td>   -0.629</td> <td>   -0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Title[T.Mrs]</th>     <td>   -0.4885</td> <td>    0.263</td> <td>   -1.858</td> <td> 0.064</td> <td>   -1.005</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Title[T.Officer]</th> <td>   -0.4960</td> <td>    0.117</td> <td>   -4.252</td> <td> 0.000</td> <td>   -0.725</td> <td>   -0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Title[T.Royalty]</th> <td>   -0.4764</td> <td>    0.207</td> <td>   -2.303</td> <td> 0.021</td> <td>   -0.882</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age</th>              <td>   -0.0042</td> <td>    0.001</td> <td>   -3.469</td> <td> 0.001</td> <td>   -0.007</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pclass_1</th>         <td>    0.2762</td> <td>    0.052</td> <td>    5.327</td> <td> 0.000</td> <td>    0.174</td> <td>    0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pclass_2</th>         <td>    0.1578</td> <td>    0.033</td> <td>    4.787</td> <td> 0.000</td> <td>    0.093</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IsMale</th>           <td>   -0.6049</td> <td>    0.251</td> <td>   -2.407</td> <td> 0.016</td> <td>   -1.098</td> <td>   -0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SibSp</th>            <td>   -0.0780</td> <td>    0.012</td> <td>   -6.369</td> <td> 0.000</td> <td>   -0.102</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HaveCabin</th>        <td>   -0.1143</td> <td>    0.048</td> <td>   -2.359</td> <td> 0.019</td> <td>   -0.209</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>43.469</td> <th>  Durbin-Watson:     </th> <td>   1.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  49.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.531</td> <th>  Prob(JB):          </th> <td>2.01e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.445</td> <th>  Cond. No.          </th> <td>1.38e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               Survived   R-squared:                       0.441\n",
       "Model:                            OLS   Adj. R-squared:                  0.434\n",
       "Method:                 Least Squares   F-statistic:                     62.99\n",
       "Date:                Thu, 16 Nov 2017   Prob (F-statistic):          4.44e-103\n",
       "Time:                        02:22:48   Log-Likelihood:                -363.02\n",
       "No. Observations:                 891   AIC:                             750.0\n",
       "Df Residuals:                     879   BIC:                             807.5\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept            1.4166      0.259      5.470      0.000       0.908       1.925\n",
       "Title[T.Miss]       -0.5878      0.261     -2.254      0.024      -1.100      -0.076\n",
       "Title[T.Mr]         -0.4899      0.071     -6.894      0.000      -0.629      -0.350\n",
       "Title[T.Mrs]        -0.4885      0.263     -1.858      0.064      -1.005       0.028\n",
       "Title[T.Officer]    -0.4960      0.117     -4.252      0.000      -0.725      -0.267\n",
       "Title[T.Royalty]    -0.4764      0.207     -2.303      0.021      -0.882      -0.070\n",
       "Age                 -0.0042      0.001     -3.469      0.001      -0.007      -0.002\n",
       "Pclass_1             0.2762      0.052      5.327      0.000       0.174       0.378\n",
       "Pclass_2             0.1578      0.033      4.787      0.000       0.093       0.222\n",
       "IsMale              -0.6049      0.251     -2.407      0.016      -1.098      -0.112\n",
       "SibSp               -0.0780      0.012     -6.369      0.000      -0.102      -0.054\n",
       "HaveCabin           -0.1143      0.048     -2.359      0.019      -0.209      -0.019\n",
       "==============================================================================\n",
       "Omnibus:                       43.469   Durbin-Watson:                   1.965\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               49.266\n",
       "Skew:                           0.531   Prob(JB):                     2.01e-11\n",
       "Kurtosis:                       3.445   Cond. No.                     1.38e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.38e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_est.info()\n",
    "est = smf.ols('Survived ~ Age + Pclass_1 + Pclass_2 + IsMale + SibSp + Title + HaveCabin', df_est).fit()\n",
    "est.summary()\n",
    "#df_est.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['Pclass','Title','Embarked'])\n",
    "#df_dummies = pd.get_dummies(df, columns=['Pclass','Embarked'])\n",
    "\n",
    "#df_dummies.head()\n",
    "df_dummies.drop(['PassengerId','Name','Sex','Ticket','Cabin'],axis=1,inplace=True)\n",
    "# Разделим тренировочную и тестовую выборку.\n",
    "\n",
    "X_train = df_dummies[df_dummies.is_test == 0].drop('is_test',axis = 1)\n",
    "X_test = df_dummies[df_dummies.is_test == 1].drop('is_test', axis = 1)\n",
    "\n",
    "# Заполнение пустых значений\n",
    "columns = X_train.columns\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0, verbose=1, copy = True)\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=columns)\n",
    "\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test_imputed = pd.DataFrame(X_test_imputed, columns=columns)\n",
    "\n",
    "# Нормировка значений\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "X_train_i_s = scaler.transform(X_train_imputed)\n",
    "X_train_i_s = pd.DataFrame(X_train_i_s, columns = columns)\n",
    "\n",
    "# Заполним тестовую выборку\n",
    "X_test_i_s = scaler.transform(imputer.transform(X_test))\n",
    "X_test_i_s = pd.DataFrame(X_test_i_s, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>IsMale</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.592481</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.565685</td>\n",
       "      <td>-0.510152</td>\n",
       "      <td>0.902587</td>\n",
       "      <td>-0.482043</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.638789</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>-0.510152</td>\n",
       "      <td>-1.107926</td>\n",
       "      <td>2.074505</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>-1.614710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.284663</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.565685</td>\n",
       "      <td>-0.510152</td>\n",
       "      <td>0.902587</td>\n",
       "      <td>-0.482043</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407926</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>-0.510152</td>\n",
       "      <td>-1.107926</td>\n",
       "      <td>-0.482043</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.407926</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.565685</td>\n",
       "      <td>-0.510152</td>\n",
       "      <td>0.902587</td>\n",
       "      <td>-0.482043</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age     SibSp     Parch      Fare    IsMale  Pclass_1  Pclass_2  \\\n",
       "0 -0.592481  0.432793 -0.473674 -0.502445  0.737695 -0.565685 -0.510152   \n",
       "1  0.638789  0.432793 -0.473674  0.786845 -1.355574  1.767767 -0.510152   \n",
       "2 -0.284663 -0.474545 -0.473674 -0.488854 -1.355574 -0.565685 -0.510152   \n",
       "3  0.407926  0.432793 -0.473674  0.420730 -1.355574  1.767767 -0.510152   \n",
       "4  0.407926 -0.474545 -0.473674 -0.486337  0.737695 -0.565685 -0.510152   \n",
       "\n",
       "   Pclass_3  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0  0.902587   -0.482043   -0.307562    0.619306  \n",
       "1 -1.107926    2.074505   -0.307562   -1.614710  \n",
       "2  0.902587   -0.482043   -0.307562    0.619306  \n",
       "3 -1.107926   -0.482043   -0.307562    0.619306  \n",
       "4  0.902587   -0.482043   -0.307562    0.619306  "
      ]
     },
     "execution_count": 1108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_i_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 1 (accuracy):0.8260381593714927 \n",
      "Best score subsample (accuracy):0.8226711560044894 \n",
      "Best score max_depth (accuracy):0.835016835016835 \n",
      "Best score learning_rate (accuracy):0.8282828282828283 \n"
     ]
    }
   ],
   "source": [
    "best_gboost_clf = get_gboost_best_model(X_train_i_s, y_train, 'accuracy',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85409652076318743"
      ]
     },
     "execution_count": 1143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(**best_gboost_clf)\n",
    "clf.fit(X_train_i_s,y_train)\n",
    "y_test = clf.predict(X_test_i_s)\n",
    "submit = pd.DataFrame(test.PassengerId)\n",
    "submit['Survived'] = y_test\n",
    "submit.to_csv('submit.csv', index = False)\n",
    "clf.score(X_train_i_s,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 1 (accuracy):0.8125701459034792 \n",
      "Fitting 2 folds for each of 21 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 2 (accuracy):0.8159371492704826 \n",
      "Best score 3 (accuracy):0.797979797979798 \n",
      "Best score 4 (accuracy):0.8372615039281706 \n",
      "Fitting 2 folds for each of 25 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 5 (accuracy):0.8148148148148148 \n",
      "Best score 6:  0.824890411649\n"
     ]
    }
   ],
   "source": [
    "best_xgboost_clf = get_xgboost_best_model(X_train_i_s, y_train,'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87317620650953987"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier(**best_xgboost_clf)\n",
    "clf.fit(X_train_i_s,y_train)\n",
    "y_test = clf.predict(X_test_i_s)\n",
    "submit = pd.DataFrame(test.PassengerId)\n",
    "submit['Survived'] = y_test\n",
    "submit.to_csv('submit.csv', index = False)\n",
    "clf.score(X_train_i_s,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'class_weight': ['balanced', None], 'penalty': ['l1', 'l2'], 'C': [0.4, 0.5, 1.0, 2.0, 2.5, 3.0, 3.5, 4.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost_clf = GradientBoostingClassifier(**best_gboost_clf)\n",
    "xgboost_clf = xgboost.XGBClassifier(**best_xgboost_clf)\n",
    "clfs = [gboost_clf, xgboost_clf]\n",
    "\n",
    "X_train_np = np.array(X_train_i_s)\n",
    "X_test_np = np.array(X_test_i_s)\n",
    "y_train_np = np.array(y_train)\n",
    "predicted = [cross_val_predict_proba(est, X_train_np, y_train_np, X_test_np) for est in clfs]\n",
    "X_train_stack = np.stack([p[0] for p in predicted], axis=1)\n",
    "X_test_stack = np.stack([p[1] for p in predicted], axis=1)\n",
    "\n",
    "cv = KFold(shuffle=True, n_splits=2)\n",
    "params = {'class_weight': ['balanced', None],\n",
    "          'penalty': ['l1', 'l2'],\n",
    "          'C': [0.4, 0.5, 1., 2., 2.5, 3., 3.5, 4.]}\n",
    "grid = GridSearchCV(LogisticRegression(), params, scoring='neg_log_loss', cv=cv)\n",
    "grid.fit(X_train_stack, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84511784511784516"
      ]
     },
     "execution_count": 1113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = grid.best_estimator_\n",
    "clf.fit(X_train_stack,y_train)\n",
    "y_test = clf.predict(X_test_stack)\n",
    "submit = pd.DataFrame(test.PassengerId)\n",
    "submit['Survived'] = y_test\n",
    "submit.to_csv('submit.csv', index = False)\n",
    "clf.score(X_train_stack,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "#features['feature'] = X_train_i_s.columns\n",
    "#features['importance'] = clf.feature_importances_\n",
    "#features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "#features.set_index('feature', inplace=True)\n",
    "#features.plot(kind='barh', figsize=(20, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SelectFromModel(clf, prefit=True,threshold=0.02)\n",
    "X_train_i_s_r = model.transform(X_train_i_s)\n",
    "\n",
    "X_train_i_s_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_i_s_r = pd.DataFrame(X_train_i_s_r, columns=X_train_i_s.columns[:12])\n",
    "X_test_i_s_r = model.transform(X_test_i_s)\n",
    "X_test_i_s_r = pd.DataFrame(X_test_i_s_r, columns=X_train_i_s.columns[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 1 (accuracy):0.8305274971941639 \n",
      "Fitting 2 folds for each of 21 candidates, totalling 42 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   14.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 2 (accuracy):0.8294051627384961 \n",
      "Best score 3 (accuracy):0.8282828282828283 \n",
      "Best score 4 (accuracy):0.8338945005611672 \n",
      "Fitting 2 folds for each of 25 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 5 (accuracy):0.8260381593714927 \n",
      "Best score 6:  0.826038159371\n"
     ]
    }
   ],
   "source": [
    "best_xgboost_clf = get_xgboost_best_model(X_train_i_s_r, y_train,'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85072951739618408"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier(**best_xgboost_clf)\n",
    "clf.fit(X_train_i_s_r,y_train)\n",
    "y_test = clf.predict(X_test_i_s_r)\n",
    "submit = pd.DataFrame(test.PassengerId)\n",
    "submit['Survived'] = y_test\n",
    "submit.to_csv('submit.csv', index = False)\n",
    "clf.score(X_train_i_s_r,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.pyll.base.Apply at 0x7f9d69f9ba20>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.quniform('max_depth', 1, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.88408057022436615, 'status': 'ok'}\n",
      "Best: \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.4, 'learning_rate': 0.03, 'max_depth': 10.0, 'min_child_weight': 4.0, 'n_estimators': 300.0, 'reg_lambda': 0.01, 'scale_pos_weight': 1.0, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# в этой функции мы проверяем, как ведёт себя модель при заданных параметрах\n",
    "def score(params):\n",
    "    #print(\"Training with params : \")\n",
    "    #print(params)\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    clf = xgboost.XGBClassifier(**params)\n",
    "    #accurs = []\n",
    "    aucs = []\n",
    "    # Для оценки качества используем KFold, который определили выше\n",
    "    for train_idx, test_idx in cv.split(X_train_i_s):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        preds = clf.predict_proba(X_test_fold)\n",
    "        auc = roc_auc_score(y_test_fold, preds[:, 1])\n",
    "        aucs.append(auc)\n",
    "        #accur = clf.score(X_test_fold, y_test_fold)\n",
    "        #accurs.append(accur)\n",
    "    auc = np.mean(aucs)\n",
    "    result = {'loss':  auc, 'status': STATUS_OK}\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# это наша главная функция, в которой мы задаём параметры\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 500, 50), # (название параметра, от, до, шаг)\n",
    "             'learning_rate' : hp.quniform('learning_rate', 0.005, 0.03, 0.005),\n",
    "             'max_depth' : hp.quniform('max_depth', 1, 10, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.1),\n",
    "             'gamma' : hp.quniform('gamma', 0.0, 0.5, 0.1),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.1),\n",
    "             'objective': 'reg:linear',\n",
    "             'silent' : 1,\n",
    "             'scale_pos_weight': hp.quniform('scale_pos_weight', 0.5, 2, 0.5),\n",
    "             'reg_alpha': 0.0,\n",
    "             'reg_lambda': hp.quniform('reg_lambda', 0.01, 0.02, 0.005),\n",
    "             }\n",
    "\n",
    "    best = fmin( score, space, algo=tpe.suggest, trials=trials, max_evals=1)\n",
    "    print('Best: ')\n",
    "    print(best)\n",
    "\n",
    "#сюда будет записана\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_7 = trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_trials  = trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_max = 1\n",
    "my_num = 0\n",
    "for i in my_trials:\n",
    "    tmp = i['result']['loss']\n",
    "    if tmp <  my_max:\n",
    "        my_num = i['tid']\n",
    "        my_max = tmp\n",
    "my_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': [0.8],\n",
       " 'gamma': [0.4],\n",
       " 'learning_rate': [0.005],\n",
       " 'max_depth': [1.0],\n",
       " 'min_child_weight': [3.0],\n",
       " 'n_estimators': [300.0],\n",
       " 'reg_lambda': [0.01],\n",
       " 'scale_pos_weight': [1.5],\n",
       " 'subsample': [0.9]}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_7 = trials.trials[my_num]['misc']['vals']\n",
    "best_params_7\n",
    "best_params_8 = best_params_7.copy()\n",
    "best_params_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'gamma': 0.4,\n",
       " 'learning_rate': 0.005,\n",
       " 'max_depth': 1,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 300,\n",
       " 'reg_lambda': 0.01,\n",
       " 'scale_pos_weight': 1.5,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_8['colsample_bytree'] = float(best_params_8['colsample_bytree'][0])\n",
    "best_params_8['gamma'] = float(best_params_8['gamma'][0])\n",
    "best_params_8['learning_rate'] = float(best_params_8['learning_rate'][0])\n",
    "best_params_8['max_depth'] = int(best_params_8['max_depth'][0])\n",
    "best_params_8['min_child_weight'] = int(best_params_8['min_child_weight'][0])\n",
    "best_params_8['n_estimators'] = int(best_params_8['n_estimators'][0])\n",
    "best_params_8['scale_pos_weight'] = float(best_params_8['scale_pos_weight'][0])\n",
    "best_params_8['reg_lambda'] = float(best_params_8['reg_lambda'][0])\n",
    "best_params_8['subsample'] = float(best_params_8['subsample'][0])\n",
    "best_params_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78226711560044893"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier(**best_params_8)\n",
    "clf.fit(X_train_i_s,y_train)\n",
    "clf.score (X_train_i_s, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем сабмит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgboost.XGBClassifier(**best_xgboost_clf)\n",
    "clf.fit(X_train_i_s,y_train)\n",
    "y_test = clf.predict(X_test_i_s)\n",
    "submit = pd.DataFrame(test.PassengerId)\n",
    "submit['Survived'] = y_test\n",
    "submit.tail()\n",
    "submit.to_csv('submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgboost_best_model(X_train,y_train, score_type, ones_ratio = 1):\n",
    "    # посчитаем соотношение между классами\n",
    "    ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    \"\"\" \n",
    "    Шаг 1: Зафиксируем learning_rate и параметры дерева и подберём n_estimators¶\n",
    "\n",
    "    Параметры:\n",
    "    \n",
    "    max_depth. Как указанов в таблице выше, обычно варьируется в интервале от 3 до 10 \n",
    "        (но от задачи к задаче значения могут меняться). В качестве начального значения обычно используют 5\n",
    "    min_child_weight. Если выборка сильно несбалансирована, то лучше выбрать значение \"1\". \n",
    "        Иначе лучше выбрать значение \"2\" и зафиксировать\n",
    "    gamma. Обычно выставляют значение в интервале от 0 до 0.2 и фиксируют. \n",
    "        В дальнейшем этот параметр всегда можно затюнить отдельно\n",
    "    subsample, colsample_bytree. Выставим 0.8 и зафиксируем. \n",
    "        Можно также проварьировать в интервале 0.5-0.9.\n",
    "    scale_pos_weight. Выставляется в зафисимости от соотношения классов в выборке и фиксируется\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        #'n_estimators': [10, 30, 50, 100, 200, 400, 600, 1000],\n",
    "        'n_estimators': [400, 600, 1000, 1200],\n",
    "        'learning_rate': [0.1, ],\n",
    "        \n",
    "        # параметры дерева\n",
    "        'max_depth': [5],\n",
    "        'min_child_weight': [2],\n",
    "        'gamma': [0.1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'scale_pos_weight': [ones_ratio],\n",
    "        \n",
    "        # параметры регуляризации\n",
    "        'reg_alpha': [0.0],\n",
    "        'reg_lambda': [1.0]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "    clf = xgboost.XGBClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_1 = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))    \n",
    "    #print('Best params: ', best_params_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 2. Подбираем параметры дерева\n",
    "\n",
    "        max_depth - будем варьировать от 3 до 10 с шагом 2\n",
    "        min_child_weight - от 1 до 6 с шагом 2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': range(3, 10, 1),\n",
    "        'min_child_weight': range(1, 6, 2)\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_1) # в качестве отправной точки возьмём модель с наилучшими параметрами предыдущего шага\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_2 = gs.best_estimator_.get_params()\n",
    "    print('Best score 2 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 3. Подбираем gamma (критерий создания поддерева)\n",
    "        gamma - от 0 до 0.5 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'gamma': [0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_2)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_3 = gs.best_estimator_.get_params()\n",
    "    print('Best score 3 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "    #print('Best params: ', best_params_3)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 4. Затюним subsample и colsample_bytree¶\n",
    "\n",
    "    subsample - от 0.5 до 1.0 с шагом 0.1\n",
    "    colsample_bytree - от 0.5 до 1.0 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'subsample': [0.5 + 0.1*i for i in range(6)],\n",
    "        'colsample_bytree': [0.5 + 0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_3)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_4 = gs.best_estimator_.get_params()\n",
    "    print('Best score 4 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_4)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 5. Регуляризация\n",
    "    reg_alpha [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    reg_lambda [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
    "        'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_4)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_5 = gs.best_estimator_.get_params()\n",
    "    print('Best score 5 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_5)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 6. Learning rate\n",
    "\n",
    "    Чем меньше у нас n_estimators в ансамбле, тем быстрее нам нужно двигаться с каждым шагом \n",
    "    (добавлением нового классификатора), т.е. делать больший learning_rate. \n",
    "    Обычно learning rate варьируют так, чтобы произведение n_estimators x learning_rate оставалось инвариантным\n",
    "    \"\"\"\n",
    "    best_params_6 = best_params_5.copy()\n",
    "    clf = xgboost.XGBClassifier(**best_params_5)\n",
    "    best_n_estimators = clf.get_params()['n_estimators'] # возьмём наилучшие значения n_estimators с предыдущего шага\n",
    "    best_learning_rate = best_params['learning_rate'] # аналогичная запись\n",
    "    invariant_composition = best_n_estimators * best_learning_rate\n",
    "    n_estimators_range = [10, 30, 100, 200, 400, 600, 800, 1000, 1200, 1400]\n",
    "\n",
    "    best_score = gs.best_score_ # возьмём наилучшее качество с предыдущего шага\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        learning_rate = invariant_composition / n_estimators\n",
    "        clf.set_params(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        #aucs = []\n",
    "        accurs = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            #preds = clf.predict_proba(X_test_fold)\n",
    "            #auc = roc_auc_score(y_test_fold, preds[:, 1])\n",
    "            #aucs.append(auc)\n",
    "            accur = clf.score(X_test_fold, y_test_fold)\n",
    "            accurs.append(accur)\n",
    "        accur = np.mean(accurs)\n",
    "        if accur > best_score:\n",
    "            best_n_estimators = n_estimators\n",
    "            best_learning_rate = learning_rate\n",
    "            best_score = accur\n",
    "    \n",
    "    best_params_6['n_estimators'] = best_n_estimators\n",
    "    best_params_6['learning_rate'] = best_learning_rate\n",
    "\n",
    "    print('Best score 6: ', best_score)\n",
    "    \n",
    "    return best_params_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gboost_best_model(X_train,y_train, score_type,N_est = 200):\n",
    "    ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        'n_estimators': [50, 100, 200, 400],\n",
    "        #'n_estimators': [N_est],\n",
    "        'max_depth' : [5],\n",
    "        'warm_start' : [True],\n",
    "        'max_features' : ['sqrt','log2',0.5, 0.7]\n",
    "    }\n",
    "    cv = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "    clf = GradientBoostingClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train,y_train)\n",
    "    \n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    # subsample \n",
    "    \n",
    "    param_grid = {\n",
    "        'subsample' : [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score subsample ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "\n",
    "    \n",
    "    # max_depth\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth' : range(3,10)\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score max_depth ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    \n",
    "\n",
    "    # learning_rate\n",
    "    \n",
    "    param_grid = {\n",
    "        'learning_rate' : [0.01,0.03, 0.05, 0.07, 0.09, 0.11]\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score learning_rate ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_titles(df):\n",
    "    \n",
    "    # we extract the title from each name\n",
    "    df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "                        \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                        }\n",
    "    \n",
    "    # we map each title\n",
    "    df['Title'] = df.Title.map(Title_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_age(df):\n",
    "    \n",
    "    grouped = df.groupby(['Sex','Pclass','Title'])\n",
    "    grouped_median = grouped.median()\n",
    "\n",
    "    #grouped_test = df.iloc[891:].groupby(['Sex','Pclass','Title'])\n",
    "    #grouped_median_test = grouped_test.median()       \n",
    "    # a function that fills the missing values of the Age variable\n",
    "    \n",
    "    def fillAges(row, grouped_median):\n",
    "        if row['Sex']=='female' and row['Pclass'] == 1:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 1, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 1, 'Mrs']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['female', 1, 'Officer']['Age']\n",
    "            elif row['Title'] == 'Royalty':\n",
    "                return grouped_median.loc['female', 1, 'Royalty']['Age']\n",
    "\n",
    "        elif row['Sex']=='female' and row['Pclass'] == 2:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 2, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 2, 'Mrs']['Age']\n",
    "\n",
    "        elif row['Sex']=='female' and row['Pclass'] == 3:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 3, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 3, 'Mrs']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 1:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 1, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 1, 'Mr']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['male', 1, 'Officer']['Age']\n",
    "            elif row['Title'] == 'Royalty':\n",
    "                return grouped_median.loc['male', 1, 'Royalty']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 2:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 2, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 2, 'Mr']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['male', 2, 'Officer']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 3:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 3, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 3, 'Mr']['Age']\n",
    "    \n",
    "    df.Age = df.apply(lambda r : fillAges(r, grouped_median_train) if np.isnan(r['Age']) \n",
    "                                                      else r['Age'], axis=1)\n",
    "    \n",
    "    #df.iloc[891:].Age = df.iloc[891:].apply(lambda r : fillAges(r, grouped_median_test) if np.isnan(r['Age']) \n",
    "     #                                                 else r['Age'], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ticket(df):\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip(), ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "    \n",
    "\n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    df['Ticket'] = df['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(df['Ticket'], prefix='Ticket')\n",
    "    df = pd.concat([df, tickets_dummies], axis=1)\n",
    "    df.drop('Ticket', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_family(df):\n",
    "\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
    "    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5<=s else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_cabin(df):\n",
    "\n",
    "    # replacing missing cabins with U (for Uknown)\n",
    "    df.Cabin.fillna('U', inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    df['Cabin'] = df['Cabin'].map(lambda c : c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')\n",
    "    \n",
    "    df = pd.concat([df,cabin_dummies], axis=1)\n",
    "    \n",
    "    df.drop('Cabin', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_predict_proba(estimator, X_train, y_train, X_test, random_state=None, n_splits=2):\n",
    "    y_test = np.zeros((len(X_test), n_splits), np.float32)\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, \n",
    "                  shuffle=True,\n",
    "                  random_state=random_state)\n",
    "\n",
    "    y_predict = np.zeros_like(y_train, np.float32)\n",
    "    for i, (train_idx, test_idx) in enumerate(kfold.split(y_train)):\n",
    "        estimator.fit(X_train[train_idx], y_train[train_idx])\n",
    "        y_predict[test_idx] = estimator.predict_proba(X_train[test_idx])[:, 1]\n",
    "        y_test[:, i] = estimator.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return y_predict, np.mean(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
