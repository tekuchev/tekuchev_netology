{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Прочитаем данные еще раз\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test  = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "# y_train = train.Survived\n",
    "# Удалим целевую функцию из train\n",
    "# train.drop('Survived',axis=1,inplace=True)\n",
    "test['Survived'] = 0\n",
    "# Пометим выборки\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "# Склеим\n",
    "df = pd.concat([train,test])\n",
    "# Заменим пол со строковой переменной на числовую\n",
    "df['IsMale'] = df.Sex.replace({'male':1, 'female':0})\n",
    "#Давим фичу HaveCabin - проверим у кого указана каюта.\n",
    "#df['HaveCabin'] = df.Cabin.isnull()\n",
    "#df.HaveCabin.replace({True : 1, False : 0},inplace=True)\n",
    "get_titles(df)\n",
    "process_age(df)\n",
    "process_ticket(df)\n",
    "process_family(df)\n",
    "process_cabin(df)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['Pclass','Title','Ticket','Cabin','Embarked'])\n",
    "#df_dummies = pd.get_dummies(df, columns=['Pclass','Embarked'])\n",
    "\n",
    "\n",
    "#df_dummies.head()\n",
    "df_dummies.drop(['Name','Sex'],axis=1,inplace=True)\n",
    "# Разделим тренировочную и тестовую выборку.\n",
    "\n",
    "X_train = df_dummies[df_dummies.is_test == 0].drop('is_test',axis = 1)\n",
    "X_test = df_dummies[df_dummies.is_test == 1].drop('is_test', axis = 1)\n",
    "\n",
    "# Заполнение пустых значений\n",
    "columns = X_train.columns\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0, verbose=1, copy = True)\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train_i = imputer.transform(X_train)\n",
    "X_train_i = pd.DataFrame(X_train_i, columns=columns)\n",
    "\n",
    "X_test_i = imputer.transform(X_test)\n",
    "X_test_i = pd.DataFrame(X_test_i, columns=columns)\n",
    "X_test_i.drop(['Survived'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Fare', 'Parch', 'PassengerId', 'SibSp', 'IsMale', 'FamilySize',\n",
       "       'Singleton', 'SmallFamily', 'LargeFamily', 'Pclass_1', 'Pclass_2',\n",
       "       'Pclass_3', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n",
       "       'Title_Officer', 'Title_Royalty', 'Ticket_A', 'Ticket_A4', 'Ticket_A5',\n",
       "       'Ticket_AQ3', 'Ticket_AQ4', 'Ticket_AS', 'Ticket_C', 'Ticket_CA',\n",
       "       'Ticket_CASOTON', 'Ticket_FC', 'Ticket_FCC', 'Ticket_Fa', 'Ticket_LINE',\n",
       "       'Ticket_LP', 'Ticket_PC', 'Ticket_PP', 'Ticket_PPP', 'Ticket_SC',\n",
       "       'Ticket_SCA3', 'Ticket_SCA4', 'Ticket_SCAH', 'Ticket_SCOW',\n",
       "       'Ticket_SCPARIS', 'Ticket_SCParis', 'Ticket_SOC', 'Ticket_SOP',\n",
       "       'Ticket_SOPP', 'Ticket_SOTONO2', 'Ticket_SOTONOQ', 'Ticket_SP',\n",
       "       'Ticket_STONO', 'Ticket_STONO2', 'Ticket_STONOQ', 'Ticket_SWPP',\n",
       "       'Ticket_WC', 'Ticket_WEP', 'Ticket_XXX', 'Cabin_A', 'Cabin_B',\n",
       "       'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T',\n",
       "       'Cabin_U', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_i.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_class_1_i = X_train_i.where(X_train_i.Pclass_1 == 1).dropna()\n",
    "y_train_class_1 = X_train_class_1_i.Survived\n",
    "X_train_class_1_i.drop(['Survived','PassengerId'], axis = 1, inplace = True)\n",
    "X_test_class_1_i = X_test_i.where(X_test_i.Pclass_1 == 1).dropna()\n",
    "X_test_class_1_PID = pd.DataFrame(X_test_class_1_i.PassengerId, index=X_test_class_1_i.index)\n",
    "X_test_class_1_i.drop(['PassengerId'], axis = 1, inplace = True)\n",
    " \n",
    "X_train_non_class_1_i = X_train_i.where(X_train_i.Pclass_1 != 1).dropna()\n",
    "y_train_non_class_1 = X_train_non_class_1_i.Survived\n",
    "X_train_non_class_1_i.drop(['PassengerId','Survived'], axis = 1, inplace = True)\n",
    "X_test_non_class_1_i = X_test_i.where(X_test_i.Pclass_1 != 1).dropna()\n",
    "\n",
    "X_test_non_class_1_PID = pd.DataFrame(X_test_non_class_1_i.PassengerId, index=X_test_non_class_1_i.index)\n",
    "X_test_non_class_1_i.drop(['PassengerId'], axis = 1, inplace = True)\n",
    "\n",
    "columns = X_train_class_1_i.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Нормировка значений для первого класса\n",
    "scaler_1 = StandardScaler()\n",
    "scaler_1.fit(X_train_class_1_i)\n",
    "X_train_class_1_i_s = scaler_1.transform(X_train_class_1_i)\n",
    "X_train_class_1_i_s = pd.DataFrame(X_train_class_1_i_s, columns = columns)\n",
    "\n",
    "# Заполним тестовую выборку\n",
    "X_test_class_1_i_s = scaler_1.transform(X_test_class_1_i)\n",
    "X_test_class_1_i_s = pd.DataFrame(X_test_class_1_i_s, columns = columns)\n",
    "\n",
    "# Нормировка значений для  всех остальных\n",
    "scaler_1 = StandardScaler()\n",
    "scaler_1.fit(X_train_non_class_1_i)\n",
    "X_train_non_class_1_i_s = scaler_1.transform(X_train_non_class_1_i)\n",
    "X_train_non_class_1_i_s = pd.DataFrame(X_train_non_class_1_i_s, columns = columns)\n",
    "\n",
    "# Заполним тестовую выборку\n",
    "X_test_non_class_1_i_s = scaler_1.transform(X_test_non_class_1_i)\n",
    "X_test_non_class_1_i_s = pd.DataFrame(X_test_non_class_1_i_s, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 1 (accuracy):0.8009259259259259 \n",
      "Best score subsample (accuracy):0.7685185185185185 \n",
      "Best score max_depth (accuracy):0.7870370370370371 \n",
      "Best score learning_rate (accuracy):0.8148148148148148 \n",
      "Best score 1 (accuracy):0.7453703703703703 \n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 105 out of 105 | elapsed:   34.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 2 (accuracy):0.7453703703703703 \n",
      "Best score 3 (accuracy):0.7453703703703703 \n",
      "Best score 4 (accuracy):0.7685185185185185 \n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:   33.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 5 (accuracy):0.7731481481481481 \n",
      "Best score 6:  0.781923890063\n"
     ]
    }
   ],
   "source": [
    "clf_class_1_gboost_params = get_gboost_best_model(X_train_class_1_i_s, y_train_class_1, 'accuracy')\n",
    "clf_class_1_xgboost_params = get_xgboost_best_model(X_train_class_1_i_s, y_train_class_1, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93981481481481477"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gboost_class_1 = GradientBoostingClassifier(**clf_class_1_gboost_params)\n",
    "clf_gboost_class_1.fit(X_train_class_1_i_s,y_train_class_1)\n",
    "y_test_gboost_class_1 = clf_gboost_class_1.predict(X_test_class_1_i_s)\n",
    "clf_gboost_class_1.score(X_train_class_1_i_s,y_train_class_1)\n",
    "\n",
    "clf_xgboost_class_1 = xgboost.XGBClassifier(**clf_class_1_xgboost_params)\n",
    "clf_xgboost_class_1.fit(X_train_class_1_i_s,y_train_class_1)\n",
    "y_test_xgboost_class_1 = clf_xgboost_class_1.predict(X_test_class_1_i_s)\n",
    "clf_gboost_class_1.score(X_train_class_1_i_s,y_train_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 1 (accuracy):0.8577777777777778 \n",
      "Best score subsample (accuracy):0.8533333333333334 \n",
      "Best score max_depth (accuracy):0.8503703703703703 \n",
      "Best score learning_rate (accuracy):0.8592592592592593 \n",
      "Best score 1 (accuracy):0.84 \n",
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 105 out of 105 | elapsed:   24.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 2 (accuracy):0.84 \n",
      "Best score 3 (accuracy):0.8429629629629629 \n",
      "Best score 4 (accuracy):0.845925925925926 \n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 125 out of 125 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 5 (accuracy):0.8429629629629629 \n",
      "Best score 6:  0.845925925926\n"
     ]
    }
   ],
   "source": [
    "clf_non_class_1_gboost_params = get_gboost_best_model(X_train_non_class_1_i_s, y_train_non_class_1, 'accuracy')\n",
    "clf_non_class_1_xgboost_params = get_xgboost_best_model(X_train_non_class_1_i_s, y_train_non_class_1, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88296296296296295"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gboost_non_class_1 = GradientBoostingClassifier(**clf_non_class_1_gboost_params)\n",
    "clf_gboost_non_class_1.fit(X_train_non_class_1_i_s,y_train_non_class_1)\n",
    "y_test_gboost_non_class_1 = clf_gboost_non_class_1.predict(X_test_non_class_1_i_s)\n",
    "clf_gboost_non_class_1.score(X_train_non_class_1_i_s,y_train_non_class_1)\n",
    "\n",
    "clf_xgboost_non_class_1 = xgboost.XGBClassifier(**clf_non_class_1_xgboost_params)\n",
    "clf_xgboost_non_class_1.fit(X_train_non_class_1_i_s,y_train_non_class_1)\n",
    "y_test_xgboost_non_class_1 = clf_xgboost_non_class_1.predict(X_test_non_class_1_i_s)\n",
    "clf_gboost_non_class_1.score(X_train_non_class_1_i_s,y_train_non_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_non_class_1_PID['Survived'] = y_test_xgboost_non_class_1\n",
    "X_test_class_1_PID['Survived'] = y_test_xgboost_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "412       1304.0       1.0\n",
       "413       1305.0       0.0\n",
       "415       1307.0       0.0\n",
       "416       1308.0       0.0\n",
       "417       1309.0       1.0"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_non_class_1_PID.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.concat([X_test_class_1_PID,X_test_non_class_1_PID])\n",
    "#submit.to_csv('submit.csv', index = False)\n",
    "submit.applymap(int).to_csv('submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые функции обработки фич взяты отсюда:\n",
    "https://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_cabin(df):\n",
    "\n",
    "    # replacing missing cabins with U (for Uknown)\n",
    "    df.Cabin.fillna('U', inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    df['Cabin'] = df['Cabin'].map(lambda c : c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')\n",
    "    \n",
    "    df = pd.concat([df,cabin_dummies], axis=1)\n",
    "    \n",
    "    df.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_family(df):\n",
    "\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
    "    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5<=s else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ticket(df):\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip(), ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "    \n",
    "\n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    df['Ticket'] = df['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(df['Ticket'], prefix='Ticket')\n",
    "    df = pd.concat([df, tickets_dummies], axis=1)\n",
    "    df.drop('Ticket', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_age(df):\n",
    "    \n",
    "    grouped = df.groupby(['Sex','Pclass','Title'])\n",
    "    grouped_median = grouped.median()\n",
    "\n",
    "    #grouped_test = df.iloc[891:].groupby(['Sex','Pclass','Title'])\n",
    "    #grouped_median_test = grouped_test.median()       \n",
    "    # a function that fills the missing values of the Age variable\n",
    "    \n",
    "    def fillAges(row, grouped_median):\n",
    "        if row['Sex']=='female' and row['Pclass'] == 1:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 1, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 1, 'Mrs']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['female', 1, 'Officer']['Age']\n",
    "            elif row['Title'] == 'Royalty':\n",
    "                return grouped_median.loc['female', 1, 'Royalty']['Age']\n",
    "\n",
    "        elif row['Sex']=='female' and row['Pclass'] == 2:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 2, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 2, 'Mrs']['Age']\n",
    "\n",
    "        elif row['Sex']=='female' and row['Pclass'] == 3:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 3, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 3, 'Mrs']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 1:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 1, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 1, 'Mr']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['male', 1, 'Officer']['Age']\n",
    "            elif row['Title'] == 'Royalty':\n",
    "                return grouped_median.loc['male', 1, 'Royalty']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 2:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 2, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 2, 'Mr']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['male', 2, 'Officer']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 3:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 3, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 3, 'Mr']['Age']\n",
    "    \n",
    "    df.Age = df.apply(lambda r : fillAges(r, grouped_median) if np.isnan(r['Age']) \n",
    "                                                      else r['Age'], axis=1)\n",
    "    \n",
    "    #df.iloc[891:].Age = df.iloc[891:].apply(lambda r : fillAges(r, grouped_median_test) if np.isnan(r['Age']) \n",
    "     #                                                 else r['Age'], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_titles(df):\n",
    "    \n",
    "    # we extract the title from each name\n",
    "    df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "                        \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                        }\n",
    "    \n",
    "    # we map each title\n",
    "    df['Title'] = df.Title.map(Title_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xgboost_best_model(X_train,y_train, score_type, ones_ratio = 1):\n",
    "    # посчитаем соотношение между классами\n",
    "    ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    \"\"\" \n",
    "    Шаг 1: Зафиксируем learning_rate и параметры дерева и подберём n_estimators¶\n",
    "\n",
    "    Параметры:\n",
    "    \n",
    "    max_depth. Как указанов в таблице выше, обычно варьируется в интервале от 3 до 10 \n",
    "        (но от задачи к задаче значения могут меняться). В качестве начального значения обычно используют 5\n",
    "    min_child_weight. Если выборка сильно несбалансирована, то лучше выбрать значение \"1\". \n",
    "        Иначе лучше выбрать значение \"2\" и зафиксировать\n",
    "    gamma. Обычно выставляют значение в интервале от 0 до 0.2 и фиксируют. \n",
    "        В дальнейшем этот параметр всегда можно затюнить отдельно\n",
    "    subsample, colsample_bytree. Выставим 0.8 и зафиксируем. \n",
    "        Можно также проварьировать в интервале 0.5-0.9.\n",
    "    scale_pos_weight. Выставляется в зафисимости от соотношения классов в выборке и фиксируется\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        #'n_estimators': [10, 30, 50, 100, 200, 400, 600, 1000],\n",
    "        'n_estimators': [400, 600, 1000, 1200],\n",
    "        'learning_rate': [0.1, ],\n",
    "        \n",
    "        # параметры дерева\n",
    "        'max_depth': [5],\n",
    "        'min_child_weight': [2],\n",
    "        'gamma': [0.1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'scale_pos_weight': [ones_ratio],\n",
    "        \n",
    "        # параметры регуляризации\n",
    "        'reg_alpha': [0.0],\n",
    "        'reg_lambda': [1.0]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    clf = xgboost.XGBClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_1 = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))    \n",
    "    #print('Best params: ', best_params_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 2. Подбираем параметры дерева\n",
    "\n",
    "        max_depth - будем варьировать от 3 до 10 с шагом 2\n",
    "        min_child_weight - от 1 до 6 с шагом 2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': range(3, 10, 1),\n",
    "        'min_child_weight': range(1, 6, 2)\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_1) # в качестве отправной точки возьмём модель с наилучшими параметрами предыдущего шага\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_2 = gs.best_estimator_.get_params()\n",
    "    print('Best score 2 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 3. Подбираем gamma (критерий создания поддерева)\n",
    "        gamma - от 0 до 0.5 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'gamma': [0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_2)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_3 = gs.best_estimator_.get_params()\n",
    "    print('Best score 3 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "    #print('Best params: ', best_params_3)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 4. Затюним subsample и colsample_bytree\n",
    "\n",
    "    subsample - от 0.5 до 1.0 с шагом 0.1\n",
    "    colsample_bytree - от 0.5 до 1.0 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'subsample': [0.5 + 0.1*i for i in range(6)],\n",
    "        'colsample_bytree': [0.5 + 0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_3)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_4 = gs.best_estimator_.get_params()\n",
    "    print('Best score 4 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_4)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 5. Регуляризация\n",
    "    reg_alpha [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    reg_lambda [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
    "        'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_4)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_5 = gs.best_estimator_.get_params()\n",
    "    print('Best score 5 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_5)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 6. Learning rate\n",
    "\n",
    "    Чем меньше у нас n_estimators в ансамбле, тем быстрее нам нужно двигаться с каждым шагом \n",
    "    (добавлением нового классификатора), т.е. делать больший learning_rate. \n",
    "    Обычно learning rate варьируют так, чтобы произведение n_estimators x learning_rate оставалось инвариантным\n",
    "    \"\"\"\n",
    "    best_params_6 = best_params_5.copy()\n",
    "    clf = xgboost.XGBClassifier(**best_params_5)\n",
    "    best_n_estimators = clf.get_params()['n_estimators'] # возьмём наилучшие значения n_estimators с предыдущего шага\n",
    "    best_learning_rate = clf.get_params()['learning_rate'] # аналогичная запись\n",
    "    invariant_composition = best_n_estimators * best_learning_rate\n",
    "    n_estimators_range = [10, 30, 100, 200, 400, 600, 800, 1000, 1200, 1400]\n",
    "\n",
    "    best_score = gs.best_score_ # возьмём наилучшее качество с предыдущего шага\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        learning_rate = invariant_composition / n_estimators\n",
    "        clf.set_params(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        #aucs = []\n",
    "        accurs = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            #preds = clf.predict_proba(X_test_fold)\n",
    "            #auc = roc_auc_score(y_test_fold, preds[:, 1])\n",
    "            #aucs.append(auc)\n",
    "            accur = clf.score(X_test_fold, y_test_fold)\n",
    "            accurs.append(accur)\n",
    "        accur = np.mean(accurs)\n",
    "        if accur > best_score:\n",
    "            best_n_estimators = n_estimators\n",
    "            best_learning_rate = learning_rate\n",
    "            best_score = accur\n",
    "    \n",
    "    best_params_6['n_estimators'] = best_n_estimators\n",
    "    best_params_6['learning_rate'] = best_learning_rate\n",
    "\n",
    "    print('Best score 6: ', best_score)\n",
    "    \n",
    "    return best_params_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gboost_best_model(X_train,y_train, score_type):\n",
    "    ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        'n_estimators': [50, 100, 200, 400],\n",
    "        #'n_estimators': [N_est],\n",
    "        'max_depth' : [5],\n",
    "        'warm_start' : [True],\n",
    "        'max_features' : ['sqrt','log2',0.5, 0.7]\n",
    "    }\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    clf = GradientBoostingClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train,y_train)\n",
    "    \n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    # subsample \n",
    "    \n",
    "    param_grid = {\n",
    "        'subsample' : [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score subsample ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "\n",
    "    \n",
    "    # max_depth\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth' : range(3,10)\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score max_depth ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    \n",
    "\n",
    "    # learning_rate\n",
    "    \n",
    "    param_grid = {\n",
    "        'learning_rate' : [0.01,0.03, 0.05, 0.07, 0.09, 0.11]\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score learning_rate ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
