{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Прочитаем данные еще раз\n",
    "train = pd.read_csv('titanic/train.csv')\n",
    "test  = pd.read_csv('titanic/test.csv')\n",
    "\n",
    "y_train = train.Survived\n",
    "# Удалим целевую функцию из train\n",
    "train.drop('Survived',axis=1,inplace=True)\n",
    "# Пометим выборки\n",
    "train['is_test'] = 0\n",
    "test['is_test'] = 1\n",
    "# Склеим\n",
    "df = pd.concat([train,test])\n",
    "# Заменим пол со строковой переменной на числовую\n",
    "df['IsMale'] = df.Sex.replace({'male':1, 'female':0})\n",
    "#Давим фичу HaveCabin - проверим у кого указана каюта.\n",
    "#df['HaveCabin'] = df.Cabin.isnull()\n",
    "#df.HaveCabin.replace({True : 1, False : 0},inplace=True)\n",
    "#get_titles(df)\n",
    "#process_age(df)\n",
    "#process_ticket(df)\n",
    "#process_family(df)\n",
    "#process_cabin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_dummies = pd.get_dummies(df, columns=['Pclass','Title','Embarked'])\n",
    "df_dummies = pd.get_dummies(df, columns=['Pclass','Embarked'])\n",
    "\n",
    "#df_dummies.head()\n",
    "df_dummies.drop(['PassengerId','Name','Sex','Ticket','Cabin'],axis=1,inplace=True)\n",
    "# Разделим тренировочную и тестовую выборку.\n",
    "\n",
    "X_train = df_dummies[df_dummies.is_test == 0].drop('is_test',axis = 1)\n",
    "X_test = df_dummies[df_dummies.is_test == 1].drop('is_test', axis = 1)\n",
    "\n",
    "# Заполнение пустых значений\n",
    "columns = X_train.columns\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0, verbose=1, copy = True)\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=columns)\n",
    "\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test_imputed = pd.DataFrame(X_test_imputed, columns=columns)\n",
    "\n",
    "# Нормировка значений\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "X_train_i_s = scaler.transform(X_train_imputed)\n",
    "X_train_i_s = pd.DataFrame(X_train_i_s, columns = columns)\n",
    "\n",
    "# Заполним тестовую выборку\n",
    "X_test_i_s = scaler.transform(imputer.transform(X_test))\n",
    "X_test_i_s = pd.DataFrame(X_test_i_s, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_cabin(df):\n",
    "\n",
    "    # replacing missing cabins with U (for Uknown)\n",
    "    df.Cabin.fillna('U', inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    df['Cabin'] = df['Cabin'].map(lambda c : c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')\n",
    "    \n",
    "    df = pd.concat([df,cabin_dummies], axis=1)\n",
    "    \n",
    "    df.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_family(df):\n",
    "\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
    "    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5<=s else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_ticket(df):\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip(), ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "    \n",
    "\n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    df['Ticket'] = df['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(df['Ticket'], prefix='Ticket')\n",
    "    df = pd.concat([df, tickets_dummies], axis=1)\n",
    "    df.drop('Ticket', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_age(df):\n",
    "    \n",
    "    grouped = df.groupby(['Sex','Pclass','Title'])\n",
    "    grouped_median = grouped.median()\n",
    "\n",
    "    #grouped_test = df.iloc[891:].groupby(['Sex','Pclass','Title'])\n",
    "    #grouped_median_test = grouped_test.median()       \n",
    "    # a function that fills the missing values of the Age variable\n",
    "    \n",
    "    def fillAges(row, grouped_median):\n",
    "        if row['Sex']=='female' and row['Pclass'] == 1:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 1, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 1, 'Mrs']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['female', 1, 'Officer']['Age']\n",
    "            elif row['Title'] == 'Royalty':\n",
    "                return grouped_median.loc['female', 1, 'Royalty']['Age']\n",
    "\n",
    "        elif row['Sex']=='female' and row['Pclass'] == 2:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 2, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 2, 'Mrs']['Age']\n",
    "\n",
    "        elif row['Sex']=='female' and row['Pclass'] == 3:\n",
    "            if row['Title'] == 'Miss':\n",
    "                return grouped_median.loc['female', 3, 'Miss']['Age']\n",
    "            elif row['Title'] == 'Mrs':\n",
    "                return grouped_median.loc['female', 3, 'Mrs']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 1:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 1, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 1, 'Mr']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['male', 1, 'Officer']['Age']\n",
    "            elif row['Title'] == 'Royalty':\n",
    "                return grouped_median.loc['male', 1, 'Royalty']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 2:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 2, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 2, 'Mr']['Age']\n",
    "            elif row['Title'] == 'Officer':\n",
    "                return grouped_median.loc['male', 2, 'Officer']['Age']\n",
    "\n",
    "        elif row['Sex']=='male' and row['Pclass'] == 3:\n",
    "            if row['Title'] == 'Master':\n",
    "                return grouped_median.loc['male', 3, 'Master']['Age']\n",
    "            elif row['Title'] == 'Mr':\n",
    "                return grouped_median.loc['male', 3, 'Mr']['Age']\n",
    "    \n",
    "    df.Age = df.apply(lambda r : fillAges(r, grouped_median_train) if np.isnan(r['Age']) \n",
    "                                                      else r['Age'], axis=1)\n",
    "    \n",
    "    #df.iloc[891:].Age = df.iloc[891:].apply(lambda r : fillAges(r, grouped_median_test) if np.isnan(r['Age']) \n",
    "     #                                                 else r['Age'], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_titles(df):\n",
    "    \n",
    "    # we extract the title from each name\n",
    "    df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "                        \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                        }\n",
    "    \n",
    "    # we map each title\n",
    "    df['Title'] = df.Title.map(Title_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gboost_best_model(X_train,y_train, score_type,N_est = 200):\n",
    "    ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        'n_estimators': [50, 100, 200, 400],\n",
    "        #'n_estimators': [N_est],\n",
    "        'max_depth' : [5],\n",
    "        'warm_start' : [True],\n",
    "        'max_features' : ['sqrt','log2',0.5, 0.7]\n",
    "    }\n",
    "    cv = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "    clf = GradientBoostingClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train,y_train)\n",
    "    \n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    # subsample \n",
    "    \n",
    "    param_grid = {\n",
    "        'subsample' : [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score subsample ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "\n",
    "    \n",
    "    # max_depth\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth' : range(3,10)\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score max_depth ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    \n",
    "\n",
    "    # learning_rate\n",
    "    \n",
    "    param_grid = {\n",
    "        'learning_rate' : [0.01,0.03, 0.05, 0.07, 0.09, 0.11]\n",
    "    }\n",
    "    clf = GradientBoostingClassifier(**best_params)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring = score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best score learning_rate ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    best_params = gs.best_estimator_.get_params()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xgboost_best_model(X_train,y_train, score_type, ones_ratio = 1):\n",
    "    # посчитаем соотношение между классами\n",
    "    ones_ratio = y_train[y_train == 1].shape[0] * 1.0 / y_train[y_train == 0].shape[0] \n",
    "    \"\"\" \n",
    "    Шаг 1: Зафиксируем learning_rate и параметры дерева и подберём n_estimators¶\n",
    "\n",
    "    Параметры:\n",
    "    \n",
    "    max_depth. Как указанов в таблице выше, обычно варьируется в интервале от 3 до 10 \n",
    "        (но от задачи к задаче значения могут меняться). В качестве начального значения обычно используют 5\n",
    "    min_child_weight. Если выборка сильно несбалансирована, то лучше выбрать значение \"1\". \n",
    "        Иначе лучше выбрать значение \"2\" и зафиксировать\n",
    "    gamma. Обычно выставляют значение в интервале от 0 до 0.2 и фиксируют. \n",
    "        В дальнейшем этот параметр всегда можно затюнить отдельно\n",
    "    subsample, colsample_bytree. Выставим 0.8 и зафиксируем. \n",
    "        Можно также проварьировать в интервале 0.5-0.9.\n",
    "    scale_pos_weight. Выставляется в зафисимости от соотношения классов в выборке и фиксируется\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        # параметры ансамбля\n",
    "        #'n_estimators': [10, 30, 50, 100, 200, 400, 600, 1000],\n",
    "        'n_estimators': [400, 600, 1000, 1200],\n",
    "        'learning_rate': [0.1, ],\n",
    "        \n",
    "        # параметры дерева\n",
    "        'max_depth': [5],\n",
    "        'min_child_weight': [2],\n",
    "        'gamma': [0.1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'scale_pos_weight': [ones_ratio],\n",
    "        \n",
    "        # параметры регуляризации\n",
    "        'reg_alpha': [0.0],\n",
    "        'reg_lambda': [1.0]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "    clf = xgboost.XGBClassifier()\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_1 = gs.best_estimator_.get_params()\n",
    "    print('Best score 1 ({0}):{1} '.format(score_type, gs.best_score_))    \n",
    "    #print('Best params: ', best_params_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 2. Подбираем параметры дерева\n",
    "\n",
    "        max_depth - будем варьировать от 3 до 10 с шагом 2\n",
    "        min_child_weight - от 1 до 6 с шагом 2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': range(3, 10, 1),\n",
    "        'min_child_weight': range(1, 6, 2)\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_1) # в качестве отправной точки возьмём модель с наилучшими параметрами предыдущего шага\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_2 = gs.best_estimator_.get_params()\n",
    "    print('Best score 2 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 3. Подбираем gamma (критерий создания поддерева)\n",
    "        gamma - от 0 до 0.5 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'gamma': [0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_2)\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_3 = gs.best_estimator_.get_params()\n",
    "    print('Best score 3 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "    #print('Best params: ', best_params_3)\n",
    "\n",
    "    \"\"\"\n",
    "    Шаг 4. Затюним subsample и colsample_bytree¶\n",
    "\n",
    "    subsample - от 0.5 до 1.0 с шагом 0.1\n",
    "    colsample_bytree - от 0.5 до 1.0 с шагом 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = {\n",
    "        'subsample': [0.5 + 0.1*i for i in range(6)],\n",
    "        'colsample_bytree': [0.5 + 0.1*i for i in range(6)]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_3)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_4 = gs.best_estimator_.get_params()\n",
    "    print('Best score 4 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_4)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 5. Регуляризация\n",
    "    reg_alpha [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    reg_lambda [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
    "        'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(**best_params_4)\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid, scoring=score_type, cv=cv, verbose=1)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_params_5 = gs.best_estimator_.get_params()\n",
    "    print('Best score 5 ({0}):{1} '.format(score_type, gs.best_score_))\n",
    "\n",
    "    #print('Best params: ', best_params_5)\n",
    "    \n",
    "    \"\"\"\n",
    "    Шаг 6. Learning rate\n",
    "\n",
    "    Чем меньше у нас n_estimators в ансамбле, тем быстрее нам нужно двигаться с каждым шагом \n",
    "    (добавлением нового классификатора), т.е. делать больший learning_rate. \n",
    "    Обычно learning rate варьируют так, чтобы произведение n_estimators x learning_rate оставалось инвариантным\n",
    "    \"\"\"\n",
    "    best_params_6 = best_params_5.copy()\n",
    "    clf = xgboost.XGBClassifier(**best_params_5)\n",
    "    best_n_estimators = clf.get_params()['n_estimators'] # возьмём наилучшие значения n_estimators с предыдущего шага\n",
    "    best_learning_rate = best_params['learning_rate'] # аналогичная запись\n",
    "    invariant_composition = best_n_estimators * best_learning_rate\n",
    "    n_estimators_range = [10, 30, 100, 200, 400, 600, 800, 1000, 1200, 1400]\n",
    "\n",
    "    best_score = gs.best_score_ # возьмём наилучшее качество с предыдущего шага\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        learning_rate = invariant_composition / n_estimators\n",
    "        clf.set_params(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        #aucs = []\n",
    "        accurs = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_train_fold, y_test_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            #preds = clf.predict_proba(X_test_fold)\n",
    "            #auc = roc_auc_score(y_test_fold, preds[:, 1])\n",
    "            #aucs.append(auc)\n",
    "            accur = clf.score(X_test_fold, y_test_fold)\n",
    "            accurs.append(accur)\n",
    "        accur = np.mean(accurs)\n",
    "        if accur > best_score:\n",
    "            best_n_estimators = n_estimators\n",
    "            best_learning_rate = learning_rate\n",
    "            best_score = accur\n",
    "    \n",
    "    best_params_6['n_estimators'] = best_n_estimators\n",
    "    best_params_6['learning_rate'] = best_learning_rate\n",
    "\n",
    "    print('Best score 6: ', best_score)\n",
    "    \n",
    "    return best_params_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
